{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Charactor-level RNN for wiki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    modified from https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "# # Plus EOS marker\n",
    "# keywords_dict = {}\n",
    "# keywords_dict['all_letters'] = string.ascii_letters + \" .,;'-\"\n",
    "# keywords_dict['n_letters'] = len(keywords_dict['all_letters']) + 1\n",
    "\n",
    "# def findFiles(path): return glob.glob(path)\n",
    "\n",
    "# # Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
    "# def unicodeToAscii(s, keywords_dict):\n",
    "#     return ''.join(\n",
    "#         c for c in unicodedata.normalize('NFD', s)\n",
    "#         if unicodedata.category(c) != 'Mn'\n",
    "#         and c in keywords_dict['all_letters']\n",
    "#     )\n",
    "\n",
    "# # Read a file and split into lines\n",
    "# def readLines(filename, keywords_dict):\n",
    "#     lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "#     return [unicodeToAscii(line, keywords_dict) for line in lines]\n",
    "\n",
    "# # Build the category_lines dictionary, a list of lines per category\n",
    "# name_list = []\n",
    "# for filename in findFiles('data/names/*.txt'):\n",
    "#     category = os.path.splitext(os.path.basename(filename))[0]\n",
    "#     name_list.extend(readLines(filename, keywords_dict))\n",
    "    \n",
    "# keywords_dict['name_vector'] = name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter\n",
    "keywords_dict ={}\n",
    "keywords_dict['hidden_size'] = 100\n",
    "keywords_dict['seq_length'] = 25\n",
    "keywords_dict['learning_rate'] = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 999922 characters, 189 unique.\n"
     ]
    }
   ],
   "source": [
    "data = open('data/wiki.txt', 'r',encoding='utf-8').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_dict['all_letters'] = ''.join(sorted(chars))\n",
    "keywords_dict['n_letters'] = len(chars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.Wxh = nn.Linear(input_size, hidden_size)\n",
    "        self.Whh = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Why = nn.Linear(hidden_size, output_size)\n",
    "        self.act = nn.Tanh()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, _input, hidden):\n",
    "        # np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh)\n",
    "        hidden = self.Wxh(_input).add(self.Whh(hidden))\n",
    "        hidden = self.act(hidden)\n",
    "        \n",
    "        output = self.Why(hidden)\n",
    "        \n",
    "        # output\n",
    "        output = self.dropout(output)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Random item from a list\n",
    "# def randomChoice(l):\n",
    "#     _a = l[random.randint(0, len(l) - 1)]\n",
    "#     print(_a)\n",
    "#     return _a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot matrix of first to last letters (not including EOS) for input\n",
    "def inputTensor(line, keywords_dict):\n",
    "    all_letters = keywords_dict['all_letters']\n",
    "    n_letters = keywords_dict['n_letters']\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for li in range(len(line)):\n",
    "        letter = line[li]\n",
    "#         print(letter)\n",
    "        tensor[li][0][all_letters.find(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# LongTensor of second letter to end (EOS) for target\n",
    "def targetTensor(line, keywords_dict):\n",
    "    all_letters = keywords_dict['all_letters']\n",
    "    n_letters = keywords_dict['n_letters']\n",
    "    letter_indexes = [all_letters.find(line[li]) for li in range(1, len(line))]\n",
    "    letter_indexes.append(n_letters - 1) # EOS\n",
    "    return torch.LongTensor(letter_indexes)\n",
    "\n",
    "def createTrainingExample(line, keywords_dict):\n",
    "#     category, line = randomTrainingPair()\n",
    "#     line = randomChoice(keywords_dict['name_vector'])\n",
    "#     category_tensor = categoryTensor(category)\n",
    "    input_line_tensor = inputTensor(line[:-1], keywords_dict)\n",
    "#     print(input_line_tensor)\n",
    "    target_line_tensor = targetTensor(line[1:], keywords_dict)\n",
    "#     print(target_line_tensor)\n",
    "\n",
    "#     return category_tensor, input_line_tensor, target_line_tensor\n",
    "    return input_line_tensor, target_line_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # keywords_dict['learning_rate'] = 0.005\n",
    "\n",
    "# def train(rnn, keywords_dict):\n",
    "#     n, p = 0, 0\n",
    "#     while p + seq_length +1 <= len(data):\n",
    "#         input_line_tensor, target_line_tensor = createTrainingExample(data[p:p+seq_length+1], keywords_dict)\n",
    "#         print(input_line_tensor)\n",
    "#         learning_rate = keywords_dict['learning_rate']\n",
    "        \n",
    "#         target_line_tensor.unsqueeze_(-1)\n",
    "#         hidden = rnn.initHidden()\n",
    "#         rnn.zero_grad()\n",
    "#         criterion = nn.NLLLoss()\n",
    "\n",
    "#         loss = 0\n",
    "\n",
    "#         for i in range(input_line_tensor.size(0)):\n",
    "#     #         output, hidden = rnn(category_tensor, input_line_tensor[i], hidden)\n",
    "#             output, hidden = rnn(input_line_tensor[i], hidden)\n",
    "\n",
    "#             l = criterion(output, target_line_tensor[i])\n",
    "# #             print(l)\n",
    "#             loss += l\n",
    "\n",
    "#         loss.backward()\n",
    "\n",
    "#         for p in rnn.parameters():\n",
    "# #             print('Waha')\n",
    "# #             print(p)\n",
    "#             p.data.add_(-learning_rate, p.grad.data)\n",
    "#         if n % 100 == 0:\n",
    "#             print(n)\n",
    "#         p += seq_length # move data pointer\n",
    "#         n += 1 # iteration counter\n",
    "#         exit()\n",
    "#     return output, loss.item() / input_line_tensor.size(0)\n",
    "\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2m 54s (5000 12%) 88.6684\n",
      "5m 49s (10000 25%) 76.5251\n",
      "8m 45s (15000 37%) 63.4525\n",
      "11m 40s (20000 50%) 82.4235\n"
     ]
    }
   ],
   "source": [
    "def running(rnn, keywords_dict):\n",
    "    seq_length = keywords_dict['seq_length']\n",
    "    rnn.to(device)\n",
    "    optimizer = torch.optim.Adam(rnn.parameters(), lr=0.001)\n",
    "#     n_iters = 10\n",
    "    print_every = 5000\n",
    "    plot_every = 500\n",
    "    all_losses = []\n",
    "    total_loss = 0 # Reset every plot_every iters\n",
    "\n",
    "    start = time.time()\n",
    "    criterion = nn.NLLLoss().to(device)\n",
    "    \n",
    "    _iter, p = 0, 0\n",
    "    while p + seq_length +1 <= len(data):\n",
    "        input_line_tensor, target_line_tensor = createTrainingExample(data[p:p+seq_length+1], keywords_dict)\n",
    "        target_line_tensor.unsqueeze_(-1)\n",
    "        \n",
    "        hidden = rnn.initHidden()\n",
    "        loss = 0\n",
    "#         loss.to(device)\n",
    "    \n",
    "        for i in range(input_line_tensor.size(0)):\n",
    "            output, hidden = rnn(input_line_tensor[i].to(device), hidden.to(device))\n",
    "            l = criterion(output, target_line_tensor[i].to(device)).to(device)\n",
    "            if loss==0:\n",
    "                loss = l\n",
    "            else:\n",
    "                loss += l\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        p += seq_length # move data pointer\n",
    "        _iter += 1 # iteration counter\n",
    "\n",
    "        total_loss += loss\n",
    "\n",
    "        if _iter % print_every == 0:\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start), _iter, _iter / (1000000/25) * 100, loss))\n",
    "\n",
    "        if _iter % plot_every == 0:\n",
    "            all_losses.append(total_loss / plot_every)\n",
    "            total_loss = 0\n",
    "    return rnn, all_losses\n",
    "n_letters = keywords_dict['n_letters']\n",
    "\n",
    "rnn = RNN(n_letters, 128, n_letters)\n",
    "rnn.to(device)\n",
    "rnn, all_losses = running(rnn, keywords_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length = 20\n",
    "\n",
    "# Sample from a category and starting letter\n",
    "def sample(start_letter='A'):\n",
    "    with torch.no_grad():  # no need to track history in sampling\n",
    "#         category_tensor = categoryTensor(category)\n",
    "        input = inputTensor(start_letter)\n",
    "        hidden = rnn.initHidden()\n",
    "\n",
    "        output_name = start_letter\n",
    "\n",
    "        for i in range(max_length):\n",
    "            output, hidden = rnn(category_tensor, input[0], hidden)\n",
    "            topv, topi = output.topk(1)\n",
    "            topi = topi[0][0]\n",
    "            if topi == n_letters - 1:\n",
    "                break\n",
    "            else:\n",
    "                letter = all_letters[topi]\n",
    "                output_name += letter\n",
    "            input = inputTensor(letter)\n",
    "\n",
    "        return output_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of w 1 w.r.t to L:  tensor(-36.)\n",
      "Gradient of w 2 w.r.t to L:  tensor(-28.)\n",
      "Gradient of w 3 w.r.t to L:  tensor(-8.)\n",
      "Gradient of w 4 w.r.t to L:  tensor(-20.)\n"
     ]
    }
   ],
   "source": [
    "from torch import FloatTensor\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "# Define the leaf nodes\n",
    "a = Variable(FloatTensor([4]))\n",
    "\n",
    "weights = [Variable(FloatTensor([i]), requires_grad=True) for i in (2, 5, 9, 7)]\n",
    "\n",
    "# unpack the weights for nicer assignment\n",
    "w1, w2, w3, w4 = weights\n",
    "\n",
    "b = w1 * a\n",
    "c = w2 * a\n",
    "d = w3 * b + w4 * c\n",
    "L = (10 - d)\n",
    "\n",
    "L.backward()\n",
    "\n",
    "for index, weight in enumerate(weights, start=1):\n",
    "    gradient, *_ = weight.grad.data\n",
    "    print(\"Gradient of w\", index , \"w.r.t to L: \", gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-6e16513a5704>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'items'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = torch.tensor([1, 2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [2],\n",
       "        [3],\n",
       "        [4]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.unsqueeze_(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
