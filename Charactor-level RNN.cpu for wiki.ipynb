{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Charactor-level RNN for wiki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    modified from https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "import time\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parameter\n",
    "keywords_dict ={}\n",
    "keywords_dict['hidden_size'] = 100\n",
    "keywords_dict['seq_length'] = 25\n",
    "keywords_dict['learning_rate'] = 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 999922 characters, 189 unique.\n"
     ]
    }
   ],
   "source": [
    "data = open('data/wiki.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keywords_dict['all_letters'] = ''.join(sorted(chars))\n",
    "keywords_dict['n_letters'] = len(chars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.Wxh = nn.Linear(input_size, hidden_size)\n",
    "        self.Whh = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Why = nn.Linear(hidden_size, output_size)\n",
    "        self.act = nn.Tanh()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, _input, hidden):\n",
    "        # np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh)\n",
    "        hidden = self.Wxh(_input).add(self.Whh(hidden))\n",
    "        hidden = self.act(hidden)\n",
    "        \n",
    "        output = self.Why(hidden)\n",
    "        \n",
    "        # output\n",
    "        output = self.dropout(output)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Random item from a list\n",
    "# def randomChoice(l):\n",
    "#     _a = l[random.randint(0, len(l) - 1)]\n",
    "#     print(_a)\n",
    "#     return _a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# One-hot matrix of first to last letters (not including EOS) for input\n",
    "def inputTensor(line, keywords_dict):\n",
    "    all_letters = keywords_dict['all_letters']\n",
    "    n_letters = keywords_dict['n_letters']\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for li in range(len(line)):\n",
    "        letter = line[li]\n",
    "#         print(letter)\n",
    "        tensor[li][0][all_letters.find(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# LongTensor of second letter to end (EOS) for target\n",
    "def targetTensor(line, keywords_dict):\n",
    "    all_letters = keywords_dict['all_letters']\n",
    "    n_letters = keywords_dict['n_letters']\n",
    "    letter_indexes = [all_letters.find(line[li]) for li in range(1, len(line))]\n",
    "    letter_indexes.append(n_letters - 1) # EOS\n",
    "    return torch.LongTensor(letter_indexes)\n",
    "\n",
    "def createTrainingExample(line, keywords_dict):\n",
    "#     category, line = randomTrainingPair()\n",
    "#     line = randomChoice(keywords_dict['name_vector'])\n",
    "#     category_tensor = categoryTensor(category)\n",
    "    input_line_tensor = inputTensor(line[:-1], keywords_dict)\n",
    "#     print(input_line_tensor)\n",
    "    target_line_tensor = targetTensor(line[1:], keywords_dict)\n",
    "#     print(target_line_tensor)\n",
    "\n",
    "#     return category_tensor, input_line_tensor, target_line_tensor\n",
    "    return input_line_tensor, target_line_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1m 26s (5000 12%) 96.6311\n",
      "2m 50s (10000 25%) 77.7080\n",
      "4m 12s (15000 37%) 63.3996\n",
      "5m 34s (20000 50%) 76.1646\n",
      "6m 55s (25000 62%) 82.0567\n",
      "8m 19s (30000 75%) 77.4184\n",
      "9m 51s (35000 87%) 71.0473\n"
     ]
    }
   ],
   "source": [
    "def running(rnn,keywords_dict):\n",
    "    seq_length = keywords_dict['seq_length']\n",
    "\n",
    "    print_every = 5000\n",
    "    plot_every = 500\n",
    "    start = time.time()\n",
    "    \n",
    "    optimizer = torch.optim.Adam(rnn.parameters(), lr=keywords_dict['learning_rate'])\n",
    "    all_losses = []\n",
    "    total_loss = 0 # Reset every plot_every iters\n",
    "    loss_fun = nn.NLLLoss()\n",
    "\n",
    "    _iter, p = 0, 0\n",
    "    epoch = 0\n",
    "    while epoch < 5:\n",
    "        if p + seq_length +1 >= len(data):\n",
    "            epoch +=1\n",
    "            p = 0\n",
    "        input_line_tensor, target_line_tensor = createTrainingExample(data[p:p+seq_length+1], keywords_dict)\n",
    "#         print(input_line_tensor)\n",
    "        target_line_tensor.unsqueeze_(-1)\n",
    "        hidden = rnn.initHidden()\n",
    "        rnn.zero_grad()\n",
    "\n",
    "        loss = 0\n",
    "\n",
    "        for i in range(input_line_tensor.size(0)):\n",
    "            output, hidden = rnn(input_line_tensor[i], hidden)\n",
    "\n",
    "            l = loss_fun(output, target_line_tensor[i])\n",
    "            loss += l\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "#         for para in rnn.parameters():\n",
    "#             para.data.add_(-learning_rate, para.grad.data)\n",
    "\n",
    "        p += seq_length # move data pointer\n",
    "        _iter += 1 # iteration counter\n",
    "\n",
    "        total_loss += loss\n",
    "\n",
    "        if _iter % print_every == 0:\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start), _iter, _iter / (1000000/25) * 100, loss))\n",
    "\n",
    "        if _iter % plot_every == 0:\n",
    "            all_losses.append(total_loss / plot_every)\n",
    "            total_loss = 0\n",
    "    return rnn, all_losses\n",
    "\n",
    "rnn = RNN(keywords_dict['n_letters'], keywords_dict['hidden_size'], keywords_dict['n_letters'])\n",
    "rnn, all_losses = running(rnn, keywords_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1m 25s (5000 12%) 97.3801\n",
      "3m 6s (10000 25%) 75.7617\n"
     ]
    }
   ],
   "source": [
    "rnn, all_losses = running(rnn, keywords_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x117db7828>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length = 100\n",
    "\n",
    "# Sample from a category and starting letter\n",
    "def sample(rnn, start_letter='<', max_length):\n",
    "    with torch.no_grad():  # no need to track history in sampling\n",
    "#         category_tensor = categoryTensor(category)\n",
    "        _input = inputTensor(start_letter)\n",
    "        hidden = rnn.initHidden()\n",
    "\n",
    "        output_name = start_letter\n",
    "\n",
    "        for i in range(max_length):\n",
    "            output, hidden = rnn(_input[0], hidden)\n",
    "            topv, topi = output.topk(1)\n",
    "            topi = topi[0][0]\n",
    "            letter = all_letters[topi]\n",
    "            outputs += letter\n",
    "            _input = inputTensor(letter)\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of w 1 w.r.t to L:  tensor(-36.)\n",
      "Gradient of w 2 w.r.t to L:  tensor(-28.)\n",
      "Gradient of w 3 w.r.t to L:  tensor(-8.)\n",
      "Gradient of w 4 w.r.t to L:  tensor(-20.)\n"
     ]
    }
   ],
   "source": [
    "from torch import FloatTensor\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "# Define the leaf nodes\n",
    "a = Variable(FloatTensor([4]))\n",
    "\n",
    "weights = [Variable(FloatTensor([i]), requires_grad=True) for i in (2, 5, 9, 7)]\n",
    "\n",
    "# unpack the weights for nicer assignment\n",
    "w1, w2, w3, w4 = weights\n",
    "\n",
    "b = w1 * a\n",
    "c = w2 * a\n",
    "d = w3 * b + w4 * c\n",
    "L = (10 - d)\n",
    "\n",
    "L.backward()\n",
    "\n",
    "for index, weight in enumerate(weights, start=1):\n",
    "    gradient, *_ = weight.grad.data\n",
    "    print(\"Gradient of w\", index , \"w.r.t to L: \", gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-6e16513a5704>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'items'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = torch.tensor([1, 2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [2],\n",
       "        [3],\n",
       "        [4]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.unsqueeze_(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [anaconda3]",
   "language": "python",
   "name": "Python [anaconda3]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
